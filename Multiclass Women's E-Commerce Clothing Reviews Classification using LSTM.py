# -*- coding: utf-8 -*-
"""Submission 1 NLP  - Almas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oLCoQBO0WzcmkA7tQfQ01VCpK1IIvRFy

**Almas Fathin Irbah**
> Dataset : "Women's E-Commerce Clothing Reviews" \
> Sumber : Kaggle \
> Link Dataset : https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews

# **Preparation**
"""

from google.colab import drive

# u/ dataframe
import pandas as pd
import re

# u/ split data
from sklearn.model_selection import train_test_split

# u/ preprocessing dan layer
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Embedding, Activation, Dense, Softmax, Dropout, SpatialDropout1D
from keras.layers.recurrent import LSTM

# u/ visualisasi plot
import matplotlib.pyplot as plt

"""## Mount to Drive"""

drive.mount('/content/drive/')

# check the path
!pwd

# Commented out IPython magic to ensure Python compatibility.
# change folder
# %cd '/content/drive/My Drive/Submission Dicoding'

# check file in folder
!ls

"""# **Cleaning the Data**"""

# read the Dataset
df = pd.read_csv("Womens Clothing E-Commerce Reviews.csv")
cols = ['Review Text', 'Recommended IND']
df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', usecols = cols)
df.head()

# simple data checking - get dataframe general information
df.info()

# simple data checking - get row and column of dataframe
print(df.shape)

# simple data checking - get columns name
print(df.columns)

# check standard missing value - multiple column
missing_data = df.isnull()
for column in missing_data.columns.values.tolist():
    print(column)
    print (missing_data[column].value_counts())
    print("")

# check standard missing value - Review Text column
df[df['Review Text'].isnull()]

# drop the Dataset
df = df.dropna()
print(df)

# replacing the row values 
df['Recommended IND'] = df['Recommended IND'].apply(str)
df['Recommended IND'].replace(to_replace = ['0','1'], value = ['Not recommended','Recommended'], inplace=True)
df.head()

"""# **Modelling and Training the Dataset**"""

# labeling the Recommended IND column
for column in ['Recommended IND']:
  dummies = pd.get_dummies(df[column])
  df[dummies.columns] = dummies

df= df.drop(columns= ['Recommended IND'])

# check the dataframe
df

# change data type
text = df['Review Text'].values
label = df[['Not recommended', 'Recommended']].values

from sklearn.model_selection import train_test_split

# split dataset 20% test / validation
genre_train, genre_test, label_train, label_test = train_test_split(text, label, test_size = 0.2)

# use tokenizer and convert to sequence
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(genre_train) 
tokenizer.fit_on_texts(genre_test)
 
sekuens_train = tokenizer.texts_to_sequences(genre_train)
sekuens_test = tokenizer.texts_to_sequences(genre_test)
 
padded_train = pad_sequences(sekuens_train) 
padded_test = pad_sequences(sekuens_test)

# modelling sequential with embedding and LSTM
model = Sequential([
    Embedding(input_dim=5000, output_dim=16),
    LSTM(64),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(2, activation='softmax')
])

# modelling compile with optimizer adam
Adam(learning_rate=0.00146, name='Adam')
model.compile(optimizer = 'Adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])

# using callback for accuracy and validation above 90%
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
      print("\nAkurasi train dan validasi didapat telah mencapai nilai > 90%!")
      self.model.stop_training = True
callbacks = myCallback()

# train the model
num_epochs = 30
history = model.fit(padded_train, label_train, epochs=num_epochs, validation_data=(padded_test, label_test), verbose=2, callbacks=[callbacks])

"""# **Plotting Accuracy**"""

# Plot Accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Plot Loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()